{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ca0f008-e6a5-47f6-a492-fc73dceffc5a",
   "metadata": {},
   "source": [
    "# 多言語の固有表現認識\n",
    "\n",
    "* ゼロショット異言語間転移\n",
    "* XLM-RoBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9fede2-e133-4e93-bb25-c68b119a70a3",
   "metadata": {},
   "source": [
    "## データセット\n",
    "\n",
    "* XTREME: Cross-lingual TRansfer Evaluation of Multilingual Encoders ベンチマーク\n",
    "* WikiANN または PAN-X\n",
    "* 多言語のWikipedia記事\n",
    "* LOC（場所）、PER（人名）、ORG（組織名）でアノテーションされている\n",
    "* `B-` 固有表現の先頭\n",
    "* `I-` 固有表現に属する連続したトークン\n",
    "* `O` どの固有表現にも属さない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333e2c11-69d4-4754-bbdb-b4e4830680f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットのconfigを取得\n",
    "from datasets import get_dataset_config_names\n",
    "\n",
    "xtreme_subsets = get_dataset_config_names(\"xtreme\")\n",
    "len(xtreme_subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c176a95-af6d-4eba-922b-7784135feca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PAN-Xに絞り込み\n",
    "panx_subsets = [s for s in xtreme_subsets if s.startswith(\"PAN\")]\n",
    "panx_subsets[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b7f8c9-8ea4-44d2-b970-602bcd0abc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ドイツ語のコーパスをロード\n",
    "from datasets import load_dataset\n",
    "\n",
    "load_dataset(\"xtreme\", name=\"PAN-X.de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a690a4-c6ef-4155-99b8-b35843969639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# スイスコーパスを模倣するためにドイツ語、フランス語、イタリア語、英語のコーパスを話者の比率でサンプリング\n",
    "from collections import defaultdict\n",
    "from datasets import DatasetDict\n",
    "\n",
    "langs = [\"de\", \"fr\", \"it\", \"en\"]\n",
    "fracs = [0.629, 0.229, 0.084, 0.059]\n",
    "panx_ch = defaultdict(DatasetDict)\n",
    "\n",
    "for lang, frac in zip(langs, fracs):\n",
    "    # 単言語コーパスをダウンロード\n",
    "    ds = load_dataset(\"xtreme\", name=f\"PAN-X.{lang}\")\n",
    "\n",
    "    # 分割をシャッフルし、話者の割合に応じてサンプリング\n",
    "    for split in ds:\n",
    "        panx_ch[lang][split] = (\n",
    "            ds[split]\n",
    "            .shuffle(seed=0)\n",
    "            .select(range(int(frac * ds[split].num_rows))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02bea24-7c32-4909-aa87-9f1057377862",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame({lang: [panx_ch[lang][\"train\"].num_rows] for lang in langs},\n",
    "             index=[\"Number of training examples\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffb9a2b-1ac3-483a-a9f4-a65800e70832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ドイツ語コーパスのデータを確認\n",
    "element = panx_ch[\"de\"][\"train\"][0]\n",
    "for key, value in element.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884f9b4f-2d6f-4457-b694-1168fd26b3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in panx_ch[\"de\"][\"train\"].features.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91033059-0fd7-4e91-b660-79cb6112ee08",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = panx_ch[\"de\"][\"train\"].features[\"ner_tags\"].feature\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb976e6e-84e5-4645-ae85-eb3c9c645410",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags.int2str(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32a5950-e338-437d-b5bf-088c7df90a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ner_tagsのIDを文字列に変換したner_tags_str列を新たに追加する\n",
    "def create_tag_names(batch):\n",
    "    return {\"ner_tags_str\": [tags.int2str(idx) for idx in batch[\"ner_tags\"]]}\n",
    "\n",
    "panx_de = panx_ch[\"de\"].map(create_tag_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14397884-82af-479f-97f2-1609e3627a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DatasetDictにner_tags_strが追加されている\n",
    "panx_de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb481664-c07e-4484-bb19-f1f74aff9413",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_example = panx_de[\"train\"][0]\n",
    "print(de_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f89f07-550d-42c5-9884-1bd4a312a4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([de_example[\"tokens\"], de_example[\"ner_tags_str\"]],\n",
    "             [\"Tokens\", \"Tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe846b70-332e-41cc-8053-2bc83457de1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# タグの数に不均衡がないか確認\n",
    "from collections import Counter\n",
    "\n",
    "split2freqs = defaultdict(Counter)\n",
    "for split, dataset in panx_de.items():\n",
    "    for row in dataset[\"ner_tags_str\"]:\n",
    "        for tag in row:\n",
    "            if tag.startswith(\"B\"):\n",
    "                tag_type = tag.split(\"-\")[1]\n",
    "                split2freqs[split][tag_type] += 1\n",
    "pd.DataFrame.from_dict(split2freqs, orient=\"index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327e23bc-cbe8-49e1-af6e-f7074fee962e",
   "metadata": {},
   "source": [
    "## 多言語Transformer\n",
    "\n",
    "* 事前学習に用いるコーパスが多言語の文書から構成されている\n",
    "* 言語を区別する明示的な情報がなくても下流タスクに対して汎化できる\n",
    "\n",
    "## XLM-RoBERTa（XLM-R）\n",
    "\n",
    "* 100言語に対してマスク言語モデルのみで事前学習\n",
    "* WikipediaのダンプデータとCommon Crawlデータを使って訓練\n",
    "* トークン化にSentencePieceを使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5952318f-d2fd-425b-8290-b1512d67e365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERTの WordPiece TokenizerとXML-RのSentencePiece Tokenizerを比較\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "bert_model_name = \"bert-base-cased\"\n",
    "xlmr_model_name = \"xlm-roberta-base\"\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "xlmr_tokenizer = AutoTokenizer.from_pretrained(xlmr_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6868fa28-5a3a-4f69-be5c-0304936097cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Jack Sparrow loves New York!\"\n",
    "bert_tokens = bert_tokenizer(text).tokens()\n",
    "xlmr_tokens = xlmr_tokenizer(text).tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896506b7-8bf2-4ed2-8be0-5b4d62000da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4831f50b-b5e7-4241-87ba-50924d70000e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlmr_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc1ec08-666e-4049-83c8-df5f160df4f9",
   "metadata": {},
   "source": [
    "## Transformerモデルクラスの詳細"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55160ef0-de08-4c6b-b4e7-9163b98fde46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import XLMRobertaConfig\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "from transformers.models.roberta.modeling_roberta import RobertaModel\n",
    "from transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03614ec8-52fa-4748-a6e8-1c16a87c7ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XLMRobertaForTokenClassification(RobertaPreTrainedModel):\n",
    "    config_class = XLMRobertaConfig\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        # ボディ部分はRobertaと共通\n",
    "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
    "\n",
    "        # ヘッダを追加\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        # ボディ部分の重みをロード\n",
    "        # RobertaPreTrainModelのメソッドで訓練済み重みをロードできる\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, \n",
    "                labels=None, **kwargs):\n",
    "        # ボディの出力を得る\n",
    "        outputs = self.roberta(input_ids, attention_mask=attention_mask,\n",
    "                               token_type_ids=token_type_ids, **kwargs)\n",
    "\n",
    "        # ヘッドで分類\n",
    "        sequence_output = self.dropout(outputs[0])\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        # Lossを計算\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        return TokenClassifierOutput(loss=loss, logits=logits, \n",
    "                                     hidden_states=outputs.hidden_states, \n",
    "                                     attentions=outputs.attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc486d2-e79c-4351-99c8-e27fbbb805f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags.names\n",
    "\n",
    "index2tag = {idx: tag for idx, tag in enumerate(tags.names)}\n",
    "tag2index = {tag: idx for idx, tag in enumerate(tags.names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a956e0-f0b9-4eca-91e9-9de6db136aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(index2tag)\n",
    "print(tag2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2780163a-211c-4281-a1ca-fd788bb53d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43af218-f7d7-4c7e-98f1-34f73df84611",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "xlmr_config = AutoConfig.from_pretrained(xlmr_model_name,\n",
    "                                         num_labels=tags.num_classes,\n",
    "                                         id2label=index2tag,\n",
    "                                         label2id=tag2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236e53a3-7dd1-4a67-96a0-15d24238465a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlmr_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f6e271-efa0-4d27-ac10-4899f13d4bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a794ac29-0fad-4a87-8f2d-1384d2e74714",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlmr_model = XLMRobertaForTokenClassification.from_pretrained(xlmr_model_name, config=xlmr_config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1173253e-b90a-4610-af07-ca65fc31944b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c298cc4-f72a-45de-b5fe-ec7165329217",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = xlmr_tokenizer.encode(text, return_tensors=\"pt\")\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08603545-c255-4bc5-9969-7e053037ec7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([xlmr_tokens, input_ids[0].numpy()], index=[\"Tokens\", \"Input IDs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959b4aa3-fad9-4eb4-aa9d-4fe1dc179f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# それぞれのトークンに対する分類ラベル\n",
    "outputs = xlmr_model(input_ids.to(device)).logits\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bb0a4f-5829-40b6-931a-cdcb6d32f865",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = torch.argmax(outputs, dim=-1)\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fdf881-f2c4-42f4-bcb4-58351b48617e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# この時点ではヘッダが訓練されていないためランダムな出力\n",
    "preds = [tags.names[p] for p in predictions[0].cpu().numpy()]\n",
    "pd.DataFrame([xlmr_tokens, preds], index=[\"tokens\", \"Tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b378a78-5c65-48ec-8018-01a07d642ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_text(text, tags, model, tokenizer):\n",
    "    # Get tokens with special characters\n",
    "    tokens = tokenizer(text).tokens()\n",
    "    # Encode the sequence into IDs\n",
    "    input_ids = xlmr_tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n",
    "    # Get predictions as distribution over 7 possible classes\n",
    "    outputs = model(input_ids)[0]\n",
    "    # Take argmax to get most likely class per token\n",
    "    predictions = torch.argmax(outputs, dim=2)\n",
    "    # Convert to DataFrame\n",
    "    preds = [tags.names[p] for p in predictions[0].cpu().numpy()]\n",
    "    return pd.DataFrame([tokens, preds], index=[\"Tokens\", \"Tags\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dcf7f9-97bf-4aa8-b671-f7dfe2607102",
   "metadata": {},
   "source": [
    "## 固有表現抽出のためのテキストトークン化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b75174-4a31-4c2e-ae95-ddd258c76e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(de_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87edc12-052d-4aae-b1f5-6bb247f46808",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = de_example[\"tokens\"]\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee01f198-f156-4d90-8e8e-075244a28b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = de_example[\"ner_tags\"]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85ef0bb-6b36-4ee7-93f3-17cb17a77ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# is_split_into_words=Trueは入力はすでに単語に分割されていることを示す\n",
    "tokenized_input = xlmr_tokenizer(de_example[\"tokens\"], is_split_into_words=True)\n",
    "tokenized_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2afae6-0aaf-4833-afe9-b08116e4ebcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# どういうトークンに分割されたか調べる\n",
    "tokens = xlmr_tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "pd.DataFrame([tokens], index=[\"Tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acc4ab7-54ab-4709-8595-9f0601848fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ids = tokenized_input.word_ids()\n",
    "pd.DataFrame([tokens, word_ids], index=[\"Tokens\", \"Word IDs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589eba88-8816-46e0-b85f-b123a49ef74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ▁がついていない単語は前のトークンの従属トークンのためラベル推定が不要なのでマスクする\n",
    "# マスクには -100 を使う\n",
    "# -100 は nn.CrossEntropyLoss の ignore_index の値\n",
    "previous_word_idx = None\n",
    "label_ids = []\n",
    "\n",
    "for word_idx in word_ids:\n",
    "    if word_idx is None or word_idx == previous_word_idx:\n",
    "        label_ids.append(-100)\n",
    "    elif word_idx != previous_word_idx:\n",
    "        label_ids.append(labels[word_idx])\n",
    "    previous_word_idx = word_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2464b5-ff96-46a6-8116-8ade0b065d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9314b1d5-2b8a-4540-b4aa-3b14750fe9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ▁がついたトークンのみ正解ラベルが割り振られていることがわかる\n",
    "labels = [index2tag[l] if l != -100 else \"IGN\" for l in label_ids]\n",
    "index = [\"Tokens\", \"Word IDs\", \"Label IDs\", \"Labels\"]\n",
    "pd.DataFrame([tokens, word_ids, label_ids, labels], index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029c4f4e-2a23-48e1-a7fe-56e2ffefd536",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    # input_idsとattention_maskが追加される\n",
    "    tokenized_inputs = xlmr_tokenizer(examples[\"tokens\"], truncation=True, \n",
    "                                      is_split_into_words=True)\n",
    "\n",
    "    # ラベルを割り当てる\n",
    "    labels = []\n",
    "    for idx, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=idx)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None or word_idx == previous_word_idx:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(label[word_idx])\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f08f9ff-3441-4e9b-97e3-eb54ee91f72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_panx_dataset(corpus):\n",
    "    return corpus.map(tokenize_and_align_labels, batched=True, remove_columns=[\"langs\", \"ner_tags\", \"tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caead2a-6953-4a3e-b580-04ccc81a9c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "panx_ch[\"de\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ac57bf-1046-4568-9bf0-426ba8d8fd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "panx_de_encoded = encode_panx_dataset(panx_ch[\"de\"])\n",
    "panx_de_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb286a52-e852-4072-b3e9-a5212005da66",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(panx_de_encoded[\"train\"][0][\"input_ids\"])\n",
    "print(panx_de_encoded[\"train\"][0][\"attention_mask\"])\n",
    "print(panx_de_encoded[\"train\"][0][\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77dc92f-79ef-4789-9344-60f5a0f26926",
   "metadata": {},
   "source": [
    "## 精度指標\n",
    "\n",
    "* 精度、再現率、F1スコアで評価する\n",
    "* 固有表現を構成するすべてのトークンで正しく予測できている必要がある\n",
    "* [seqeval](https://github.com/chakki-works/seqeval) というリポジトリが有用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4354d808-574d-47c5-a311-d92c1322313b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a63f36-269b-4f04-b9dc-554702c065dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import classification_report\n",
    "\n",
    "y_true = [[\"O\", \"O\", \"O\", \"B-MISC\", \"I-MISC\", \"I-MISC\", \"O\"],\n",
    "          [\"B-PER\", \"I-PER\", \"O\"]]\n",
    "\n",
    "y_pred = [[\"O\", \"O\", \"B-MISC\", \"I-MISC\", \"I-MISC\", \"I-MISC\", \"O\"],\n",
    "          [\"B-PER\", \"I-PER\", \"O\"]]\n",
    "\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53019b79-f907-416b-951f-a02069a64777",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def align_predictions(predictions, label_ids):\n",
    "    preds = np.argmax(predictions, axis=2)\n",
    "    batch_size, seq_len = preds.shape\n",
    "    labels_list, preds_list = [], []\n",
    "\n",
    "    for batch_idx in range(batch_size):\n",
    "        example_labels, example_preds = [], []\n",
    "        for seq_idx in range(seq_len):\n",
    "            # Ignore label IDs = -100\n",
    "            if label_ids[batch_idx, seq_idx] != -100:\n",
    "                example_labels.append(index2tag[label_ids[batch_idx][seq_idx]])\n",
    "                example_preds.append(index2tag[preds[batch_idx][seq_idx]])\n",
    "\n",
    "        labels_list.append(example_labels)\n",
    "        preds_list.append(example_preds)\n",
    "\n",
    "    return preds_list, labels_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e745434-01ab-4c7e-b1a9-a4ecb7edaede",
   "metadata": {},
   "source": [
    "## XLM-RoBERTaのFine-tuning\n",
    "\n",
    "* 現状はヘッド部分がランダムな重みの状態なのでFine-tuningが必要\n",
    "* ドイツ語でFine-tuningする\n",
    "* フランス語、イタリア語、英語でのゼロショット言語間転移性能を評価する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c447b5-ff76-4029-9a3b-7f285b19da4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "num_epochs = 3\n",
    "batch_size = 24\n",
    "logging_steps = len(panx_de_encoded[\"train\"]) // batch_size\n",
    "model_name = f\"{xlmr_model_name}-finetuned-panx-de\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_name,\n",
    "    log_level=\"error\", \n",
    "    num_train_epochs=num_epochs, \n",
    "    per_device_train_batch_size=batch_size, \n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    evaluation_strategy=\"epoch\", \n",
    "    save_steps=1e6,\n",
    "    weight_decay=0.01,\n",
    "    disable_tqdm=False, \n",
    "    logging_steps=logging_steps,\n",
    "    push_to_hub=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c7e406-f2dd-4c93-a228-121ea0c01c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    y_pred, y_true = align_predictions(eval_pred.predictions, eval_pred.label_ids)\n",
    "    return {\"f1\": f1_score(y_true, y_pred)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d274280-a248-430a-936c-ce011cdf8998",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(xlmr_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7403db76-0022-4f01-b16f-b46cd81e16d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 新しいモデルを返す\n",
    "def model_init():\n",
    "    return (XLMRobertaForTokenClassification\n",
    "            .from_pretrained(xlmr_model_name, config=xlmr_config)\n",
    "            .to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5646a45d-cfd8-4677-b375-6753e621d6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 新しいバージョンではエラーが出るので環境変数を設定\n",
    "%env TOKENIZERS_PARALLELISM=falsea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34b67bd-a221-42d3-bec4-ac17a6d3f244",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(model_init=model_init,\n",
    "                  args=training_args, \n",
    "                  data_collator=data_collator,\n",
    "                  compute_metrics=compute_metrics,\n",
    "                  train_dataset=panx_de_encoded[\"train\"],\n",
    "                  eval_dataset=panx_de_encoded[\"validation\"], \n",
    "                  tokenizer=xlmr_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4dd3a0-9e83-410f-8452-c0e184ee75d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16a7ed7-5b59-4a49-bd3e-68f3aa7e23d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub(commit_message=\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32556b07-f270-494c-8ee4-384d42258a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_de = \"Jeff Dean ist ein Informatiker bei Google in Kalifornien\"\n",
    "tag_text(text_de, tags, trainer.model, xlmr_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162a68e6-c633-464c-a610-46106daf8f93",
   "metadata": {},
   "source": [
    "## エラー分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7def9bc2-c24e-46ad-8f58-5d518caf5fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "def forward_pass_with_label(batch):\n",
    "    # Convert dict of lists to list of dicts suitable for data collator\n",
    "    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
    "    # Pad inputs and labels and put all tensors on device\n",
    "    batch = data_collator(features)\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "    with torch.no_grad():\n",
    "        # Pass data through model  \n",
    "        output = trainer.model(input_ids, attention_mask)\n",
    "        # Logit.size: [batch_size, sequence_length, classes]\n",
    "        # Predict class with largest logit value on classes axis\n",
    "        predicted_label = torch.argmax(output.logits, axis=-1).cpu().numpy()\n",
    "    # Calculate loss per token after flattening batch dimension with view\n",
    "    loss = cross_entropy(output.logits.view(-1, 7), \n",
    "                         labels.view(-1), reduction=\"none\")\n",
    "    # Unflatten batch dimension and convert to numpy array\n",
    "    loss = loss.view(len(input_ids), -1).cpu().numpy()\n",
    "\n",
    "    return {\"loss\":loss, \"predicted_label\": predicted_label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efed4c08-2088-4474-8c78-8c9e6b26601c",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_set = panx_de_encoded[\"validation\"]\n",
    "valid_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398f68c0-9eb3-491d-aedc-fba722378dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = valid_set[0:3]\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe52783-6434-44d8-875a-cfb9d613a670",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b73028e-8281-41a4-b3b3-3ff4386c9b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = data_collator(features)\n",
    "input_ids = batch[\"input_ids\"].to(device)\n",
    "attention_mask = batch[\"attention_mask\"].to(device)\n",
    "labels = batch[\"labels\"].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fdd2a4-ff99-4021-8b8c-06b272443034",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = trainer.model(input_ids, attention_mask)\n",
    "# (batch, seqlen, class)\n",
    "output[\"logits\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354ee8ec-66aa-4266-9313-379ba0d70e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_label = torch.argmax(output.logits, axis=-1)\n",
    "predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118afb3f-424b-4ea5-a509-0bcc9ce711a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ddf5f5-5e03-47ff-8998-9218978a6e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# バッチと系列長をまとめる\n",
    "output.logits.view(-1, 7).shape, labels.view(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08910e7-9e27-4dbe-9262-679613c18fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = cross_entropy(output.logits.view(-1, 7), labels.view(-1), reduction=\"none\")\n",
    "loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d623532-2d03-46d7-b4a1-091a008d81e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_set = panx_de_encoded[\"validation\"]\n",
    "valid_set = valid_set.map(forward_pass_with_label, batched=True, batch_size=32)\n",
    "df = valid_set.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56ff449-659c-47a7-ac2e-fd5a4a987776",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3206372-09b4-4084-876a-448e76379705",
   "metadata": {},
   "outputs": [],
   "source": [
    "index2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ae616d-5283-4e50-8811-3d8aed1225f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "index2tag[-100] = \"IGN\"\n",
    "df[\"input_tokens\"] = df[\"input_ids\"].apply(lambda x: xlmr_tokenizer.convert_ids_to_tokens(x))\n",
    "df[\"predicted_label\"] = df[\"predicted_label\"].apply(lambda x: [index2tag[i] for i in x])\n",
    "df[\"labels\"] = df[\"labels\"].apply(lambda x: [index2tag[i] for i in x])\n",
    "df['loss'] = df.apply(lambda x: x['loss'][:len(x['input_ids'])], axis=1)\n",
    "df['predicted_label'] = df.apply(lambda x: x['predicted_label'][:len(x['input_ids'])], axis=1)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1773eaed-0b92-4777-b674-71302f050fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tokens = df.apply(pd.Series.explode)\n",
    "df_tokens = df_tokens.query(\"labels != 'IGN'\")\n",
    "df_tokens[\"loss\"] = df_tokens[\"loss\"].astype(float).round(2)\n",
    "df_tokens.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0296330-1fa3-4c6a-baac-ca7fdeb6657b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_tokens.groupby(\"input_tokens\")[[\"loss\"]]\n",
    "    .agg([\"count\", \"mean\", \"sum\"])\n",
    "    .droplevel(level=0, axis=1)  # Get rid of multi-level columns\n",
    "    .sort_values(by=\"sum\", ascending=False)\n",
    "    .reset_index()\n",
    "    .round(2)\n",
    "    .head(10)\n",
    "    .T\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1db4992-150a-4bbd-b00c-282070ff7520",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_tokens.groupby(\"labels\")[[\"loss\"]] \n",
    "    .agg([\"count\", \"mean\", \"sum\"])\n",
    "    .droplevel(level=0, axis=1)\n",
    "    .sort_values(by=\"mean\", ascending=False)\n",
    "    .reset_index()\n",
    "    .round(2)\n",
    "    .T\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53481b4f-720e-464c-9ebf-6c037eb745f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(y_preds, y_true, labels):\n",
    "    cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n",
    "    plt.title(\"Normalized confusion matrix\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a90a92-e365-43ab-ade4-94721673de87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plot_confusion_matrix(df_tokens[\"labels\"], df_tokens[\"predicted_label\"], tags.names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696064e1-6b78-47f2-92b3-e71f52a9bbe0",
   "metadata": {},
   "source": [
    "## 言語間転移"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b465690-c66e-4bb5-8d63-c597a447cd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_f1_score(trainer, dataset):\n",
    "    return trainer.predict(dataset).metrics[\"test_f1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55381824-7640-4fae-a8f5-a7b2c7306d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores = defaultdict(dict)\n",
    "f1_scores[\"de\"][\"de\"] = get_f1_score(trainer, panx_de_encoded[\"test\"])\n",
    "f1_scores[\"de\"][\"de\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610d4b78-c0b0-4ae7-a8a5-a2d342e1bb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_fr = \"Jeff Dean est informaticien chez Google en Kalifornien\"\n",
    "tag_text(text_fr, tags, trainer.model, xlmr_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040fd838-cf08-46dc-88da-ffe72d447ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_lang_performance(lang, trainer):\n",
    "    panx_ds = encode_panx_dataset(panx_ch[lang])\n",
    "    return get_f1_score(trainer, panx_ds[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94b140b-8833-4cc6-a2d0-929d9c4c03b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# de => frに転移した結果を評価\n",
    "f1_scores[\"de\"][\"fr\"] = evaluate_lang_performance(\"fr\", trainer)\n",
    "f1_scores[\"de\"][\"fr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb0502d-8d09-4b0b-b1a5-69bbd1fbcc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores[\"de\"][\"it\"] = evaluate_lang_performance(\"it\", trainer)\n",
    "f1_scores[\"de\"][\"it\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3313fa-9ef1-45b7-b8da-ab99fa8c9a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores[\"de\"][\"en\"] = evaluate_lang_performance(\"en\", trainer)\n",
    "f1_scores[\"de\"][\"en\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efeaea0-12ba-42cd-be6c-9548f7312e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# フランス語でスクラッチから訓練したときの精度はどう変わるか？\n",
    "def train_on_subset(dataset, num_samples):\n",
    "    train_ds = dataset[\"train\"].shuffle(seed=42).select(range(num_samples))\n",
    "    valid_ds = dataset[\"validation\"]\n",
    "    test_ds = dataset[\"test\"]\n",
    "    training_args.logging_step = len(train_ds) // batch_size\n",
    "    \n",
    "    trainer = Trainer(model_init=model_init,\n",
    "                      args=training_args, \n",
    "                      data_collator=data_collator,\n",
    "                      compute_metrics=compute_metrics,\n",
    "                      train_dataset=train_ds,\n",
    "                      eval_dataset=valid_ds,\n",
    "                      tokenizer=xlmr_tokenizer)\n",
    "    trainer.train()\n",
    "    if training_args.push_to_hub:\n",
    "        trainer.push_to_hub(commit_message=\"Training completed!\")\n",
    "    \n",
    "    f1_score = get_f1_score(trainer, test_ds)\n",
    "    return pd.DataFrame.from_dict({\n",
    "        \"num_samples\": [len(train_ds)],\n",
    "        \"f1_score\": [f1_score]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666fa6bb-eec1-4fa9-b067-807709e9e14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "panx_ch[\"fr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48b1beb-d813-42df-9abb-68566d982a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "panx_fr_encoded = encode_panx_dataset(panx_ch[\"fr\"])\n",
    "panx_fr_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6367ff12-b969-4e6d-a2c9-d57563c8b244",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args.push_to_hub = False\n",
    "metrics_df = train_on_subset(panx_fr_encoded, 250)\n",
    "metrics_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
