{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ca0f008-e6a5-47f6-a492-fc73dceffc5a",
   "metadata": {},
   "source": [
    "# 多言語の固有表現認識\n",
    "\n",
    "* ゼロショット異言語間転移\n",
    "* XLM-RoBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9fede2-e133-4e93-bb25-c68b119a70a3",
   "metadata": {},
   "source": [
    "## データセット\n",
    "\n",
    "* XTREME: Cross-lingual TRansfer Evaluation of Multilingual Encoders ベンチマーク\n",
    "* WikiANN または PAN-X\n",
    "* 多言語のWikipedia記事\n",
    "* LOC（場所）、PER（人名）、ORG（組織名）でアノテーションされている\n",
    "* `B-` 固有表現の先頭\n",
    "* `I-` 固有表現に属する連続したトークン\n",
    "* `O` どの固有表現にも属さない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333e2c11-69d4-4754-bbdb-b4e4830680f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットのconfigを取得\n",
    "from datasets import get_dataset_config_names\n",
    "\n",
    "xtreme_subsets = get_dataset_config_names(\"xtreme\")\n",
    "len(xtreme_subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c176a95-af6d-4eba-922b-7784135feca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PAN-Xに絞り込み\n",
    "panx_subsets = [s for s in xtreme_subsets if s.startswith(\"PAN\")]\n",
    "panx_subsets[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b7f8c9-8ea4-44d2-b970-602bcd0abc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ドイツ語のコーパスをロード\n",
    "from datasets import load_dataset\n",
    "\n",
    "load_dataset(\"xtreme\", name=\"PAN-X.de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a690a4-c6ef-4155-99b8-b35843969639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# スイスコーパスを模倣するためにドイツ語、フランス語、イタリア語、英語のコーパスを話者の比率でサンプリング\n",
    "from collections import defaultdict\n",
    "from datasets import DatasetDict\n",
    "\n",
    "langs = [\"de\", \"fr\", \"it\", \"en\"]\n",
    "fracs = [0.629, 0.229, 0.084, 0.059]\n",
    "panx_ch = defaultdict(DatasetDict)\n",
    "\n",
    "for lang, frac in zip(langs, fracs):\n",
    "    # 単言語コーパスをダウンロード\n",
    "    ds = load_dataset(\"xtreme\", name=f\"PAN-X.{lang}\")\n",
    "\n",
    "    # 分割をシャッフルし、話者の割合に応じてサンプリング\n",
    "    for split in ds:\n",
    "        panx_ch[lang][split] = (\n",
    "            ds[split]\n",
    "            .shuffle(seed=0)\n",
    "            .select(range(int(frac * ds[split].num_rows))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02bea24-7c32-4909-aa87-9f1057377862",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame({lang: [panx_ch[lang][\"train\"].num_rows] for lang in langs},\n",
    "             index=[\"Number of training examples\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffb9a2b-1ac3-483a-a9f4-a65800e70832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ドイツ語コーパスのデータを確認\n",
    "element = panx_ch[\"de\"][\"train\"][0]\n",
    "for key, value in element.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884f9b4f-2d6f-4457-b694-1168fd26b3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in panx_ch[\"de\"][\"train\"].features.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91033059-0fd7-4e91-b660-79cb6112ee08",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = panx_ch[\"de\"][\"train\"].features[\"ner_tags\"].feature\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb976e6e-84e5-4645-ae85-eb3c9c645410",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags.int2str(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32a5950-e338-437d-b5bf-088c7df90a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ner_tagsのIDを文字列に変換したner_tags_str列を新たに追加する\n",
    "def create_tag_names(batch):\n",
    "    return {\"ner_tags_str\": [tags.int2str(idx) for idx in batch[\"ner_tags\"]]}\n",
    "\n",
    "panx_de = panx_ch[\"de\"].map(create_tag_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14397884-82af-479f-97f2-1609e3627a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DatasetDictにner_tags_strが追加されている\n",
    "panx_de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb481664-c07e-4484-bb19-f1f74aff9413",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_example = panx_de[\"train\"][0]\n",
    "print(de_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f89f07-550d-42c5-9884-1bd4a312a4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([de_example[\"tokens\"], de_example[\"ner_tags_str\"]],\n",
    "             [\"Tokens\", \"Tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe846b70-332e-41cc-8053-2bc83457de1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# タグの数に不均衡がないか確認\n",
    "from collections import Counter\n",
    "\n",
    "split2freqs = defaultdict(Counter)\n",
    "for split, dataset in panx_de.items():\n",
    "    for row in dataset[\"ner_tags_str\"]:\n",
    "        for tag in row:\n",
    "            if tag.startswith(\"B\"):\n",
    "                tag_type = tag.split(\"-\")[1]\n",
    "                split2freqs[split][tag_type] += 1\n",
    "pd.DataFrame.from_dict(split2freqs, orient=\"index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327e23bc-cbe8-49e1-af6e-f7074fee962e",
   "metadata": {},
   "source": [
    "## 多言語Transformer\n",
    "\n",
    "* 事前学習に用いるコーパスが多言語の文書から構成されている\n",
    "* 言語を区別する明示的な情報がなくても下流タスクに対して汎化できる\n",
    "\n",
    "## XLM-RoBERTa（XLM-R）\n",
    "\n",
    "* 100言語に対してマスク言語モデルのみで事前学習\n",
    "* WikipediaのダンプデータとCommon Crawlデータを使って訓練\n",
    "* トークン化にSentencePieceを使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5952318f-d2fd-425b-8290-b1512d67e365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERTの WordPiece TokenizerとXML-RのSentencePiece Tokenizerを比較\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "bert_model_name = \"bert-base-cased\"\n",
    "xlmr_model_name = \"xlm-roberta-base\"\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "xlmr_tokenizer = AutoTokenizer.from_pretrained(xlmr_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6868fa28-5a3a-4f69-be5c-0304936097cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Jack Sparrow loves New York!\"\n",
    "bert_tokens = bert_tokenizer(text).tokens()\n",
    "xlmr_tokens = xlmr_tokenizer(text).tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896506b7-8bf2-4ed2-8be0-5b4d62000da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4831f50b-b5e7-4241-87ba-50924d70000e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlmr_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc1ec08-666e-4049-83c8-df5f160df4f9",
   "metadata": {},
   "source": [
    "## Transformerモデルクラスの詳細"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55160ef0-de08-4c6b-b4e7-9163b98fde46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import XLMRobertaConfig\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "from transformers.models.roberta.modeling_roberta import RobertaModel\n",
    "from transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03614ec8-52fa-4748-a6e8-1c16a87c7ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XLMRobertaForTokenClassification(RobertaPreTrainedModel):\n",
    "    config_class = XLMRobertaConfig\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        # ボディ部分はRobertaと共通\n",
    "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
    "\n",
    "        # ヘッダを追加\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        # ボディ部分の重みをロード\n",
    "        # RobertaPreTrainModelのメソッドで訓練済み重みをロードできる\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, \n",
    "                labels=None, **kwargs):\n",
    "        # ボディの出力を得る\n",
    "        outputs = self.roberta(input_ids, attention_mask=attention_mask,\n",
    "                               token_type_ids=token_type_ids, **kwargs)\n",
    "\n",
    "        # ヘッドで分類\n",
    "        sequence_output = self.dropout(outputs[0])\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        # Lossを計算\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        return TokenClassifierOutput(loss=loss, logits=logits, \n",
    "                                     hidden_states=outputs.hidden_states, \n",
    "                                     attentions=outputs.attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc486d2-e79c-4351-99c8-e27fbbb805f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags.names\n",
    "\n",
    "index2tag = {idx: tag for idx, tag in enumerate(tags.names)}\n",
    "tag2index = {tag: idx for idx, tag in enumerate(tags.names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a956e0-f0b9-4eca-91e9-9de6db136aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(index2tag)\n",
    "print(tag2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2780163a-211c-4281-a1ca-fd788bb53d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43af218-f7d7-4c7e-98f1-34f73df84611",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "xlmr_config = AutoConfig.from_pretrained(xlmr_model_name,\n",
    "                                         num_labels=tags.num_classes,\n",
    "                                         id2label=index2tag,\n",
    "                                         label2id=tag2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236e53a3-7dd1-4a67-96a0-15d24238465a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlmr_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f6e271-efa0-4d27-ac10-4899f13d4bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a794ac29-0fad-4a87-8f2d-1384d2e74714",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlmr_model = XLMRobertaForTokenClassification.from_pretrained(xlmr_model_name, config=xlmr_config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1173253e-b90a-4610-af07-ca65fc31944b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c298cc4-f72a-45de-b5fe-ec7165329217",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = xlmr_tokenizer.encode(text, return_tensors=\"pt\")\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08603545-c255-4bc5-9969-7e053037ec7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([xlmr_tokens, input_ids[0].numpy()], index=[\"Tokens\", \"Input IDs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959b4aa3-fad9-4eb4-aa9d-4fe1dc179f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# それぞれのトークンに対する分類ラベル\n",
    "outputs = xlmr_model(input_ids.to(device)).logits\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bb0a4f-5829-40b6-931a-cdcb6d32f865",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = torch.argmax(outputs, dim=-1)\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fdf881-f2c4-42f4-bcb4-58351b48617e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# この時点ではヘッダが訓練されていないためランダムな出力\n",
    "preds = [tags.names[p] for p in predictions[0].cpu().numpy()]\n",
    "pd.DataFrame([xlmr_tokens, preds], index=[\"tokens\", \"Tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b378a78-5c65-48ec-8018-01a07d642ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_text(text, tags, model, tokenizer):\n",
    "    # Get tokens with special characters\n",
    "    tokens = tokenizer(text).tokens()\n",
    "    # Encode the sequence into IDs\n",
    "    input_ids = xlmr_tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n",
    "    # Get predictions as distribution over 7 possible classes\n",
    "    outputs = model(input_ids)[0]\n",
    "    # Take argmax to get most likely class per token\n",
    "    predictions = torch.argmax(outputs, dim=2)\n",
    "    # Convert to DataFrame\n",
    "    preds = [tags.names[p] for p in predictions[0].cpu().numpy()]\n",
    "    return pd.DataFrame([tokens, preds], index=[\"Tokens\", \"Tags\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dcf7f9-97bf-4aa8-b671-f7dfe2607102",
   "metadata": {},
   "source": [
    "## 固有表現抽出のためのテキストトークン化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b75174-4a31-4c2e-ae95-ddd258c76e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_examp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87edc12-052d-4aae-b1f5-6bb247f46808",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = de_example[\"tokens\"]\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee01f198-f156-4d90-8e8e-075244a28b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = de_example[\"ner_tags\"]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85ef0bb-6b36-4ee7-93f3-17cb17a77ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# is_split_into_words=Trueは入力はすでに単語に分割されていることを示す\n",
    "tokenized_input = xlmr_tokenizer(de_example[\"tokens\"], is_split_into_words=True)\n",
    "tokenized_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2afae6-0aaf-4833-afe9-b08116e4ebcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# どういうトークンに分割されたか調べる\n",
    "tokens = xlmr_tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "pd.DataFrame([tokens], index=[\"Tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acc4ab7-54ab-4709-8595-9f0601848fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ids = tokenized_input.word_ids()\n",
    "pd.DataFrame([tokens, word_ids], index=[\"Tokens\", \"Word IDs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3156dd3-da16-49d2-a369-9af0fe39d50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ▁がついていない単語は前のトークンの従属トークンのためラベル推定が不要なのでマスクする\n",
    "# マスクには -100 を使う\n",
    "# -100 は nn.CrossEntropyLoss の ignore_index の値"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589eba88-8816-46e0-b85f-b123a49ef74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide_output\n",
    "previous_word_idx = None\n",
    "label_ids = []\n",
    "\n",
    "for word_idx in word_ids:\n",
    "    if word_idx is None or word_idx == previous_word_idx:\n",
    "        label_ids.append(-100)\n",
    "    elif word_idx != previous_word_idx:\n",
    "        label_ids.append(labels[word_idx])\n",
    "    previous_word_idx = word_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2464b5-ff96-46a6-8116-8ade0b065d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9314b1d5-2b8a-4540-b4aa-3b14750fe9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ▁がついたトークンのみ正解ラベルが割り振られていることがわかる\n",
    "labels = [index2tag[l] if l != -100 else \"IGN\" for l in label_ids]\n",
    "index = [\"Tokens\", \"Word IDs\", \"Label IDs\", \"Labels\"]\n",
    "pd.DataFrame([tokens, word_ids, label_ids, labels], index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029c4f4e-2a23-48e1-a7fe-56e2ffefd536",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    # input_idsとattention_maskが追加される\n",
    "    tokenized_inputs = xlmr_tokenizer(examples[\"tokens\"], truncation=True, \n",
    "                                      is_split_into_words=True)\n",
    "\n",
    "    # ラベルを割り当てる\n",
    "    labels = []\n",
    "    for idx, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=idx)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None or word_idx == previous_word_idx:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(label[word_idx])\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f08f9ff-3441-4e9b-97e3-eb54ee91f72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_panx_dataset(corpus):\n",
    "    return corpus.map(tokenize_and_align_labels, batched=True, remove_columns=[\"langs\", \"ner_tags\", \"tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caead2a-6953-4a3e-b580-04ccc81a9c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "panx_ch[\"de\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ac57bf-1046-4568-9bf0-426ba8d8fd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "panx_de_encoded = encode_panx_dataset(panx_ch[\"de\"])\n",
    "panx_de_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb286a52-e852-4072-b3e9-a5212005da66",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(panx_de_encoded[\"train\"][0][\"input_ids\"])\n",
    "print(panx_de_encoded[\"train\"][0][\"attention_mask\"])\n",
    "print(panx_de_encoded[\"train\"][0][\"labels\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
