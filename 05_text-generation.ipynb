{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64b68b14-6468-4240-ab38-93c38566ee24",
   "metadata": {},
   "source": [
    "# テキスト生成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bc07b1-f103-4363-9fb5-9d9cdf0d4bd2",
   "metadata": {},
   "source": [
    "## 貪欲法によるデコード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd119a96-d8cd-488a-ada9-3d3f85274c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-2をロード\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f46ed60-9cda-4fe2-99c9-95fe10c9a1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f562c9c1-ebf5-457a-a680-0d2ad5adfa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2-xl\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28d01be-96ac-4f18-b797-8261a4721bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_txt = \"Transformers are the\"\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e132b75-21fd-476f-b04e-43061e25ff5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = []\n",
    "n_steps = 8\n",
    "choices_per_step = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236e827b-c2e4-4371-8b01-81e8a193093a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb4fae9-3bc9-46ee-a4e3-cce9b7d0146d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in input_ids[0]:\n",
    "    print(k, tokenizer.decode(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e854fb-e61c-4aa7-a15d-96c902d85f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(input_ids=input_ids)\n",
    "output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407bb1e5-d654-4c59-872d-c42d1fc47bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c338b8f-3231-438f-bcb8-ee239764bcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_logits = output.logits[0, -1, :]\n",
    "next_token_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0740e22c-ffdd-45cc-8bf3-0a6ea567ee2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
    "next_token_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f537e767-1a67-4064-a09f-5cde0076d798",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True)\n",
    "sorted_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e8378a-e456-4825-925e-d8faff87e965",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_id = sorted_ids[0]\n",
    "token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8131fc0b-3023-43c0-85c5-bd5058df6dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_prob = next_token_probs[token_id]\n",
    "token_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dc695d-fb3e-4202-bb60-f67055d159f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_choice = (f\"{tokenizer.decode(token_id)} ({100 * token_prob: .2f}%)\")\n",
    "token_choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0942faf6-4b75-4ad5-ab66-4ae678801cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim=-1)\n",
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3add26b6-b494-40a9-b6be-cbd27d230cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in input_ids[0]:\n",
    "    print(k, tokenizer.decode(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095b0f9f-413d-4500-b68d-71df04099095",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(input_ids=input_ids)\n",
    "output.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570634d8-8d00-435d-bd01-37093b848f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 貪欲法でデコード\n",
    "# 動作を理解するために手動で生成\n",
    "with torch.no_grad():\n",
    "    for _ in range(n_steps):\n",
    "        iteration = dict()\n",
    "        iteration[\"Input\"] = tokenizer.decode(input_ids[0])\n",
    "        output = model(input_ids=input_ids)\n",
    "        # Select logits of the first batch and the last token and apply softmax\n",
    "        next_token_logits = output.logits[0, -1, :]\n",
    "        next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
    "        sorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True)\n",
    "        # Store tokens with highest probabilities\n",
    "        for choice_idx in range(choices_per_step):\n",
    "            token_id = sorted_ids[choice_idx]\n",
    "            token_prob = next_token_probs[token_id].cpu().numpy()\n",
    "            token_choice = (\n",
    "                f\"{tokenizer.decode(token_id)} ({100 * token_prob:.2f}%)\"\n",
    "            )\n",
    "            iteration[f\"Choice {choice_idx+1}\"] = token_choice\n",
    "        # Append predicted next token to input\n",
    "        input_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim=-1)\n",
    "        iterations.append(iteration)\n",
    "        \n",
    "pd.DataFrame(iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b548f01-89ea-4bb5-bc0f-d651f6361e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generateメソッドを使う\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output = model.generate(input_ids, max_new_tokens=n_steps, do_sample=False)\n",
    "tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72771f8-4933-477d-9697-6a90489876ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# もう少し長い例文で試す\n",
    "max_length = 128\n",
    "input_txt = \"\"\"In a shocking finding, scientist discovered \\\n",
    "a herd of unicorns living in a remote, previously unexplored \\\n",
    "valley, in the Andes Mountains. Even more surprising to the \\\n",
    "researchers was the fact that the unicorns spoke perfect English.\\n\\n\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48fcfa5-360b-4499-9618-3b7ef38c652f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 反復的な出力系列を生成している => 貪欲法でよくある欠点\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output_greedy = model.generate(input_ids, max_length=max_length, do_sample=False)\n",
    "tokenizer.decode(output_greedy[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81eb8ee4-376a-4694-8969-e798aee031f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_greedy[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65838c9c-b2e4-4a3c-9aa3-55464016403a",
   "metadata": {},
   "source": [
    "## ビームサーチによるデコード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a504b462-b0b0-4234-8bac-1067afb16e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各入力に対する次のトークン候補の確率を計算する\n",
    "# 各時刻で出力されるlogitsを正規化することで確率分布にできる\n",
    "\n",
    "# 生成したこの128トークンの系列の生成確率を求めたい\n",
    "labels = output_greedy\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27e17f0-ae67-4c92-a768-d125a2da97e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各時刻でのトークンの生成確率を求める\n",
    "output = model(labels)\n",
    "output.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e4f0c4-02fd-4720-bdcb-622627f9b0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = output.logits[:, :-1, :]\n",
    "labels = labels[:, 1:]\n",
    "logits.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb133136-57ec-4d96-a561-f8dfe8f13f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# アンダーフローしないように対数確率にする\n",
    "logp = F.log_softmax(logits, dim=-1)\n",
    "logp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519a2187-89b4-4a6a-b0cb-a1632b99dfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.unsqueeze(2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfcf07f-fe92-4715-b016-e8bf19e97f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logpから生成トークンの確率を収集して足し合わせる\n",
    "logp_label = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)\n",
    "logp_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463bd1f9-dc0c-49fe-9631-d7854c7e1ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 系列の対数尤度は和をとればよい\n",
    "seq_log_prob = torch.sum(logp_label[:, 47:])\n",
    "seq_log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f146d4e5-7515-4770-9174-e1674cd2d31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上の処理を関数にまとめると\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def log_probs_from_logits(logits, labels):\n",
    "    logp = F.log_softmax(logits, dim=-1)\n",
    "    logp_label = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)\n",
    "    return logp_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09d9827-e5cc-47ae-8b65-d23edd90ca4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_logprob(model, labels, input_len=0):\n",
    "    with torch.no_grad():\n",
    "        output = model(labels)\n",
    "        log_probs = log_probs_from_logits(\n",
    "            output.logits[:, :-1, :], labels[:, 1:])\n",
    "        seq_log_prob = torch.sum(log_probs[:, input_len:])\n",
    "    return seq_log_prob.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4593d2-decd-4c10-863d-ab3227c9f5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 貪欲法で生成した系列に対する対数尤度\n",
    "logp = sequence_logprob(model, output_greedy, input_len=len(input_ids[0]))\n",
    "print(tokenizer.decode(output_greedy[0]))\n",
    "print(f\"log_prob: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c073e9ec-5b2f-412f-8c94-4e1a4367fa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ビームサーチで生成した系列に対する対数尤度\n",
    "# 貪欲法に比べて対数尤度が大きくなっており、よりありえそうな系列を生成していることがわかる\n",
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams=5, do_sample=False)\n",
    "logp = sequence_logprob(model, output_beam, input_len=len(input_ids[0]))\n",
    "print(tokenizer.decode(output_beam[0]))\n",
    "print(f\"log_prob: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8d97ab-1e37-4c59-a109-f6d4b9189a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_repeat_ngram_sizeを指定することで以前に出現したn-gramが出現しないようにする\n",
    "# 文章の繰り返しが防げる\n",
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams=5, do_sample=False, no_repeat_ngram_size=2)\n",
    "logp = sequence_logprob(model, output_beam, input_len=len(input_ids[0]))\n",
    "print(tokenizer.decode(output_beam[0]))\n",
    "print(f\"log_prob: {logp:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
